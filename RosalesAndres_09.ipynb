{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maxmiztion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries we will use:\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# two additionals for the Lestrade code:\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. write a simulator as a positive control**\n",
    "\n",
    "Write a script that simulates N=1000000 observed read counts, following the model specified above, for any Arc locus structure (i.e. lengths Li) and any transcript abundances τ. (You can assume that there are 10 segments and isoforms, though my version of this script will allow that to vary too.)\n",
    "\n",
    "Use you script to generate a test data set for known model parameters τ,L that you've chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read the provided data table with pandas and store it for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas to store our data into a datafile, \n",
    "df_data = pd.read_table('w09-data.out.txt', # File name\n",
    "                            delim_whitespace=True, # Whether it is whitespace delimited\n",
    "                            skiprows=1, # Number of rows to skip\n",
    "                            header = None, # Row that has the header (None since we are going to provide it ourselves)\n",
    "                            names = ['arc','nothing','length','segments'], # Column names\n",
    "                            index_col=0 # Set the row names using the first column: gene name in this case\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea whether this was an intended step for the problem set, or whether I am doing it unecessarily long, but here I will do some tricks to clean out the data and get it ready to use in our generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observed read counts to simulate:\n",
    "## Including it here because we need it in order to divide the transcript abundances into nucleotide abundances\n",
    "N = 1000000\n",
    "\n",
    "# Cleaning up the data with a few tricks:\n",
    "df_data = df_data.reset_index()  # we do this to avoid using the gene names as indexes and be able to store in list \n",
    "data = df_data.values.tolist()\n",
    "\n",
    "del data[10]\n",
    "\n",
    "for i in range(10):\n",
    "    del data[i][1]\n",
    "  \n",
    "for i in range(10):\n",
    "    del data[10+i][2]\n",
    "    \n",
    "for i in range(10):\n",
    "    del data[10+i][2]\n",
    "    \n",
    "arcs = data[0:10]\n",
    "nus = data[10:20]\n",
    "\n",
    "# Getting lengths into one list:\n",
    "lengths = []\n",
    "for i in arcs:\n",
    "    int_i = int(i[1])\n",
    "    lengths.append(int_i)\n",
    "\n",
    "# Getting our nus ready\n",
    "for i in nus:\n",
    "    i[1] = float(i[1])/N\n",
    "\n",
    "n_1 = []  # empty list for nu letters\n",
    "n_2 = []  # empty list for nu values\n",
    "for i in nus: \n",
    "    n_1.append(i[0])\n",
    "    n_2.append(i[1])   \n",
    " \n",
    "nu = [n_1, n_2]\n",
    "\n",
    "a_1 = []  # empty list for arcs[0]\n",
    "a_2 = []  # empty list for arcs[1]\n",
    "a_3 = []  # empty list for arcs[2]\n",
    "\n",
    "for i in arcs:\n",
    "    a_1.append(i[0])\n",
    "    a_2.append(i[1]) \n",
    "    a_3.append(i[2]) \n",
    "    \n",
    "lens = [a_1, a_2, a_3]\n",
    "\n",
    "# Storing number of genes here:\n",
    "number_genes = len(lens[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by writing a function that generates N read counts. This will give us our test data set for known model parameters τ,L. This is our **generative model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change from for loop to np.random.choice(N) to make it more efficient\n",
    "\n",
    "def simulate_reads(N, theta):\n",
    "    '''\n",
    "    Given N (the number of reads to sample),\n",
    "    and theta, which includes nu (nucleotide abundance) and L (transcript lengths),\n",
    "    Simulate reads\n",
    "    '''    \n",
    "    # Unpacking our two parameters from theta to use them in our function: \n",
    "    nus = theta[0]                   # nucleotide abundances     \n",
    "    L = theta[1]                     # list storing lengths for each transctipt\n",
    "\n",
    "    # Create empty list of reads:\n",
    "    reads = []\n",
    "    \n",
    "    # writing it as a for loop:\n",
    "    for i in range(N):\n",
    "        # 1. Choose a transcript G=i with probability V_i:\n",
    "        transcript = np.random.choice(nus[0], 1, p = nus[1])  # choose a transcript based on the nucleotide abundances\n",
    "        index = nus[0].index(transcript)                      # index for the transcript (0-9)\n",
    "        index_seg = arcs[index][2]                            # find the segments that compose that transcript\n",
    "        list_of_letters = list(index_seg)                     # separate segments into list of letters\n",
    "        \n",
    "        # 2. Choose a segment from that transcript with equal probability:\n",
    "        segment = np.random.choice(list_of_letters, 1)        # pick uniformly from the list of segments in the transcript\n",
    "        step = list(segment)                                  # extra step to convert from np.array to list\n",
    "        read = step[0]                                        # appending and also removing \"double brackets\"\n",
    "        # 3. Generate a read (counting that segment and mapping a read to it)\n",
    "        ## simplified model means the segment maps to our read directly: \n",
    "        reads.append(read)                                 \n",
    "\n",
    "    # Return list of size N with our reads:\n",
    "    return reads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our function let's run it with our desired parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observed read counts to simulate:\n",
    "N = 1000000\n",
    "\n",
    "# Storing both parameters in variable \"theta\" (previously computed above)\n",
    "theta = [nu, lens]\n",
    "\n",
    "# Running our function and storing the output\n",
    "sim_reads = simulate_reads(N, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. calculate the log likelihood**\n",
    "\n",
    "Write a function that calculates the log likelihood: the log probability of the observed data (the observed read counts rk) if the model and its parameters were known (i.e. τ,L), for a given locus structure of Arc.\n",
    "\n",
    "Calculate and show the log likelihood of one of your generated test data sets, for your known parameter values.\n",
    "\n",
    "Use Lestrade's approach (and his code, if you like) to estimate abundances of each Arc isoform in your test data set. Compare to your true τ. (Terrible, right?) Calculate and show the log likelihood given what Lestrade would estimate for the τ, compared to the log likelihood for the true τ in your positive control.\n",
    "\n",
    "You should observe that the true τ parameter values give a much better log likelihood than the Lestrade et al. estimates, because the Lestrade et al. estimates are poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing our function to calculate the log likelihood for a given locus structure of Arc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep adding and then just log at the end\n",
    "## we want one P for every read, and then log each one and add all of them\n",
    "\n",
    "def log_likelihood(R, theta):\n",
    "    '''\n",
    "    Given R (reads) and \n",
    "    theta (nucleotide abundances and transcript lengths),\n",
    "    Calculate the total log likelihood of the model\n",
    "    '''\n",
    "    # Unpacking our parameters from theta to use them in our function with ease: \n",
    "    abundances = theta[0][1]         # our 10 nucleotide abundances\n",
    "    arcs = theta[1][2]               # segments of the 10 arcs (i.e. 'ABCD')\n",
    "    lengths = theta[1][1]            # length of the 10 arcs\n",
    "\n",
    "    ll = 0\n",
    "    for read in R:\n",
    "        joint = 0  # joint initial\n",
    "        # For each read, compute a log likelihood for each of the 10 arcs:\n",
    "        j = 0 # counter variable that will range 0-9 to keep track of which arc we are dealing with\n",
    "        for arc in arcs:\n",
    "            if read in arc:\n",
    "                pre_joint = (1/int(lengths[j]))*abundances[j]\n",
    "                joint = joint + pre_joint\n",
    "            j=j+1\n",
    "        ll = ll + np.log(joint)\n",
    "    \n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's specify our parameters for our log likelihood function and run it with our previously simulated reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total log likelihood summed over the ten arcs: \n",
      "2140469.7194456058\n"
     ]
    }
   ],
   "source": [
    "# As a reminder, theta is: \n",
    "theta = [nu, lens]\n",
    "\n",
    "# Using our function to compute our list of ll's\n",
    "ll = log_likelihood(sim_reads, theta)\n",
    "\n",
    "# Getting our total log likelihood:\n",
    "print('Total log likelihood summed over the ten arcs: ')\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Lestrade's code to estimate abundances for each Arc isoform in the test data set we generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115419.66666667  52349.33333333  81691.66666667 130411.83333333\n",
      " 150381.16666667 131258.16666667 102686.5         83889.5\n",
      "  79741.16666667  72171.        ]\n",
      "Arc1        0.085\n",
      "Arc2        0.077\n",
      "Arc3        0.080\n",
      "Arc4        0.096\n",
      "Arc5        0.111\n",
      "Arc6        0.129\n",
      "Arc7        0.151\n",
      "Arc8        0.123\n",
      "Arc9        0.078\n",
      "Arc10       0.071\n"
     ]
    }
   ],
   "source": [
    "# Parse the input file.\n",
    "with open('w09-data.out.txt') as f:\n",
    "    #   The first line is \"The <n> transcripts of the sand mouse Arc locus\"\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'^The (\\d+) transcripts', line)\n",
    "    T     = int(match.group(1))\n",
    "\n",
    "    # The next T lines are \n",
    "    #   <Arcn>  <true_tau> <L> <structure>\n",
    "    # tau's may be present, or obscured (\"xxxxx\")\n",
    "    tau       = np.zeros(T)\n",
    "    L         = np.zeros(T).astype(int)\n",
    "    tau_known = True   # until we see otherwise\n",
    "    for i in range(T):\n",
    "        fields    = f.readline().split()\n",
    "        if fields[1] == \"xxxxx\":\n",
    "            tau_known = False\n",
    "        else:\n",
    "            tau[i] = float(fields[1])\n",
    "        L[i]      = int(fields[2])\n",
    "\n",
    "    # after a blank line,\n",
    "    # 'The <n> read sequences':\n",
    "    line  = f.readline()\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'The (\\d+) read sequences', line)\n",
    "    N     = int(match.group(1))\n",
    "\n",
    "    # the next T lines are \n",
    "    #  <read a-j> <count>\n",
    "    r = np.zeros(T).astype(int)\n",
    "    for k in range(T):\n",
    "        fields = f.readline().split()\n",
    "        r[k]   = fields[1]\n",
    "\n",
    "\n",
    "S = T    # S = R = T : there are T transcripts (Arc1..Arc10), S segments (A..J), R reads (a..j)\n",
    "R = T\n",
    "Slabel   = list(string.ascii_uppercase)[:S]               # ['A'..'J']        : the upper case labels for Arc locus segments \n",
    "Tlabel   = [ \"Arc{}\".format(d) for d in range(1,T+1) ]    # ['Arc1'..'Arc10'] : the labels for Arc transcript isoforms\n",
    "Rlabel   = list(string.ascii_lowercase)[:T]               # ['a'..'j']        : lower case labels for reads\n",
    "\n",
    "\n",
    "# Count how often each segment A..J is used in the isoforms i\n",
    "# We'll use that to split observed read counts across the isoforms\n",
    "# that they might have come from.\n",
    "#\n",
    "segusage = np.zeros(S).astype(int)\n",
    "for i in range(T):\n",
    "    for j in range(i,i+L[i]): \n",
    "        segusage[j%S] += 1\n",
    "\n",
    "\n",
    "# Naive analysis:\n",
    "#\n",
    "c  = np.zeros(T)\n",
    "for i in range(T):\n",
    "    for k in range(i,i+L[i]):\n",
    "        c[i] += (1.0 / float(segusage[k%S])) * float(r[k%S])  # For each read k, assume read k-> segment j,\n",
    "                                                              # and assign 1/usage count to each transcript\n",
    "                                                              # that contains segment j.\n",
    "Z       = np.sum(c)\n",
    "est_nu  = np.divide(c, Z)       # nucleotide abundance\n",
    "\n",
    "print(c)\n",
    "\n",
    "est_tau = np.divide(est_nu, L)  # convert to TPM, transcript abundance\n",
    "est_tau = np.divide(est_tau, np.sum(est_tau))\n",
    "\n",
    "# Print a table of the resulting estimates for tau\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:5.3f}\".format(Tlabel[i], est_tau[i]))\n",
    "tau = nu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this above results to our true abundances tau, we realize Lestrade's are terrible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arc1        0.089\n",
      "Arc2        0.069\n",
      "Arc3        0.088\n",
      "Arc4        0.100\n",
      "Arc5        0.057\n",
      "Arc6        0.086\n",
      "Arc7        0.198\n",
      "Arc8        0.213\n",
      "Arc9        0.061\n",
      "Arc10       0.039\n"
     ]
    }
   ],
   "source": [
    "# Our abundances:\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:5.3f}\".format(Tlabel[i], nu[1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do some formatting here to get the Lestrade data in the correct format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lestrade_nus = [nu[0], est_tau]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our log likelihood function defined above to calculate Lestrade's total log likelihood using his estimated taus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total log likelihood for Lestrade: \n",
      "2162952.708061929\n"
     ]
    }
   ],
   "source": [
    "# The theta parameters for Lestrade: \n",
    "theta = [lestrade_nus, lens]\n",
    "\n",
    "# The reads are the ones from our generated test data set:\n",
    "R = sim_reads\n",
    "\n",
    "# Using our function to compute Lestrade's list of ll's\n",
    "lestrade_ll = log_likelihood(R, theta)\n",
    "\n",
    "# Getting Lestrade's total log likelihood:\n",
    "print('Total log likelihood for Lestrade: ')\n",
    "print(lestrade_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to our model, which uses the true τ parameter values, we can see that Lestrade's total log likelihood is a  larger number, which means it is much less likely than ours (remember we are in log space!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22482.9886163231\n"
     ]
    }
   ],
   "source": [
    "# Showing difference between our log likelihood and Lestrade's (remember we are in log space)\n",
    "ll_diff = lestrade_ll - ll\n",
    "    \n",
    "# Displaying the differences:\n",
    "print(ll_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. estimate isoform abundances by EM**\n",
    "Write a script that estimates unknown isoform abundances τi for each isoform Arc1..Arc10, given read counts rk and the structure of the Arc locus including the lengths Li, using expectation maximization.\n",
    "\n",
    "Apply your script to the data in the Lestrade et al. supplementary data file. Show your estimated τi.\n",
    "\n",
    "What do you think of Lestrade et al.'s conclusion that all 10 mRNA transcripts are expressed at about the same level? What are the most abundant two transcripts, and how much of the population to they account for? What are the least abundant two transcripts?\n",
    "\n",
    "- **Step 1**: initialize $\\theta$ to anything (can be random)\n",
    "\n",
    "\n",
    "- **Step 2**: (expectation) calculate $c_i = \\sum_n P(G_n=i|R_n,\\theta)$\n",
    "\n",
    "\n",
    "- **Step 3**: (maximization) calculate updated $\\theta_i = c_i/N$\n",
    "\n",
    "\n",
    "- **Step 4**: repeat EM until converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to set our initial parameters theta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'],\n",
       "  [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]],\n",
       " [['Arc1',\n",
       "   'Arc2',\n",
       "   'Arc3',\n",
       "   'Arc4',\n",
       "   'Arc5',\n",
       "   'Arc6',\n",
       "   'Arc7',\n",
       "   'Arc8',\n",
       "   'Arc9',\n",
       "   'Arc10'],\n",
       "  ['4', '2', '3', '4', '4', '3', '2', '2', '3', '3'],\n",
       "  ['ABCD', 'BC', 'CDE', 'DEFG', 'EFGH', 'FGH', 'GH', 'HI', 'IJA', 'JAB']]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing initial parameters theta 'randomly'. \n",
    "# This is our initial guess at the probabilty of each read to belong to each transcript\n",
    "\n",
    "theta[0][1] = [1/number_genes] * number_genes\n",
    "\n",
    "# reminder of what the variable theta holds:\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strategy for EM:\n",
    "\n",
    "# Idea:\n",
    "   # We want to run our EM until our LL converges and is optimized\n",
    "\n",
    "\n",
    "# Now do expectation step:\n",
    "\n",
    "  # How many reads do we expect mapped to each transcript? c_i \n",
    "\n",
    "  # then update by: counts assigned to transcript A/total number of counts\n",
    "\n",
    "  # after update calculate ll again and check for convergence (ends until it converges)\n",
    "    \n",
    "    # each read has some probabilty of belonging to each cluster\n",
    "    # first calculate the ten probabilities or counts for each read. and then sum over all N reads\n",
    "    # final result should be an array of 10 abundances\n",
    "\n",
    "  # expectation step is in picture; then you update parameters by normalizing the counts for each transcript\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second step is to write our function for expectation, which should return a list of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(R, theta):\n",
    "    '''\n",
    "    Given R (reads) and \n",
    "    theta (nucleotide abundances and transcript lengths),\n",
    "    Run EM until convergence\n",
    "    '''\n",
    "    # Unpacking our parameters from theta to use them in our function with ease: \n",
    "    abundances = theta[0][1]         # our 10 nucleotide abundances\n",
    "    arcs = theta[1][2]               # segments of the 10 arcs (i.e. 'ABCD')\n",
    "    lengths = theta[1][1]            # length of the 10 arcs\n",
    "    N = len(R)                       # number of reads\n",
    "\n",
    "    ## this is all groundwork for the marginalization:\n",
    "    \n",
    "    # Summing over all genes to get probability:    \n",
    "    list_of_strings = []     # empty list to store all the transcripts in (i.e. 'ABC' or 'IAJ')  \n",
    "    for i in arcs:    # loop through each arc\n",
    "        list_of_strings.append(list(i))    # add all of them to one list and separate into chars for easy counting\n",
    "\n",
    "    list_of_strings = np.sum(list_of_strings) # this makes it such that they are all one long list   \n",
    "\n",
    "    # Making a list with each count to loop through. It will look like: [A,B,C,D,E,F,G,H,I,J]\n",
    "    unique_segments = list(dict.fromkeys(list_of_strings))  # remove duplicates to create this list (and avoid hardcoding)\n",
    "\n",
    "    # Empty list to store counts in:\n",
    "    a_counts = []\n",
    "\n",
    "    # loop through our arcs and count ocurrences of each segment:\n",
    "    for i in unique_segments: # for each segment [A,B,C,D,E,F,G,H,I,J] count ocurrences in the ten arcs\n",
    "        a_counts.append(list_of_strings.count(i))\n",
    "\n",
    "    # we also want to store the total number of segments:\n",
    "    total_num_segs = np.sum(a_counts)\n",
    "\n",
    "    seg_probs = []  # empty list for probabilities of each of the ten segments \n",
    "    for i in a_counts:    # probability for each segment (ocurrences of segment/total number of segments)\n",
    "        seg_probs.append(i/total_num_segs)\n",
    "    \n",
    "    \n",
    "    ## Let's write the actual loop now:\n",
    "    \n",
    "    counts = np.zeros(len(abundances))    # array of size 10 to hold our counts\n",
    "    probs = np.zeros(N,dtype=object)     # array of size N to hold 10 probs for each read\n",
    "\n",
    "    # do this for all reads:\n",
    "    i = 0\n",
    "    for read in R:\n",
    "        joints = []  # temporary list to store 10 joints for each read\n",
    "        # For each read, compute a log likelihood for each of the 10 arcs:\n",
    "        j = 0 # counter variable that will range 0-9 to keep track of which arc we are dealing with\n",
    "        for arc in arcs:\n",
    "            if read in arc:\n",
    "                numerator = (1/int(lengths[j]))*abundances[j]   # probability calculation\n",
    "                denominator =  seg_probs[j]* np.sum(abundances)  # numerator summed over all transcripts\n",
    "                joints.append(numerator/denominator)\n",
    "            else:\n",
    "                joints.append(0)   # if no probability\n",
    "            j=j+1\n",
    "\n",
    "        probs[i] = joints\n",
    "        i+=1\n",
    "    # Sum counts over all reads for each of the 10 transcripts:\n",
    "    for i in range(len(probs)):\n",
    "        for j in range(len(probs[0])):\n",
    "            counts[j] = counts[j] + probs[i][j]\n",
    "    \n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for our third step we write a function for maximization, which takes in counts as an input and returns an updated u (nucleotide abundances). These will be the new parameters theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization(C):\n",
    "    '''\n",
    "    Given C (read counts) and \n",
    "    Caluclate the updated nu (nucleotide abundances)\n",
    "    '''\n",
    "    # Normalizing all the counts by dividing\n",
    "    for i in range(len(counts)):\n",
    "        counts[i] = counts[i]/N\n",
    "    \n",
    "    # changing the name\n",
    "    abundances = counts\n",
    "    \n",
    "    return abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our expectation maximization until convergence.\n",
    "We will also calculate the log likelihood after every EM pass so that we keep track of how we are doing.\n",
    "\n",
    "\n",
    "Important note: Unlike k-means where we ran with 20 different initial conditions, here we are only running with one set of initial conditions because Li and Dewey proved there is only one optimum\n",
    "\n",
    "I am doing it in two different ways:\n",
    "1. As in Sean's solution, running the EM a large number of times (defined as Sean by 1000)\n",
    "2. Using the more elegant convergence tresholds\n",
    "\n",
    "\n",
    "First way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we want to run this EM:\n",
    "n = 1000\n",
    "\n",
    "for i in range(n):  # absolute value of our treshold which is a percent change in ll \n",
    "    counts = expectation(sim_reads, theta) # obtain our counts by using our expectation function\n",
    "    abundances = maximization(counts)  # normalize counts into abundances by using our maximization function\n",
    "    theta[0][1] = abundances  # updating our theta parameters with the latest abundances\n",
    "    ll = log_likelihood(sim_reads, theta)  # also calculating our log likelihood for every EM run\n",
    "    \n",
    "# this EM takes a bit to run, but it does converge to the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second way: \n",
    "\n",
    "(leaving it commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting arbitrary initial ll's to make sure our loop runs:\n",
    "#old_ll = 10000000\n",
    "#ll = 0 \n",
    "\n",
    "# Setting a convergence treshold as a percentage change in ll\n",
    "#treshold = 10\n",
    "\n",
    "#while abs(ll - old_ll)  > treshold:  # absolute value of our treshold which is a percent change in ll \n",
    "    #counts = expectation(sim_reads, theta)\n",
    "    #abundances = maximization(counts)\n",
    "    #theta[0][1] = abundances  # updating our theta parameters with the latest abundances\n",
    "    #ll = log_likelihood(sim_reads, theta)\n",
    "    \n",
    "    #return abundances,ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used nucleotide abundances above, we want to convert our results back to transcript abundances for our final step. This will help us look at the total numbers to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nucleotide abundance to transcript abundance\n",
    "taus = []  # empty list to store our abundances in after converting\n",
    "for i in abundances:\n",
    "    taus.append(i*N)\n",
    "    \n",
    "# Take a look at our estimates for the true taus:\n",
    "print (taus)\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think of Lestrade et al.'s conclusion that all 10 mRNA transcripts are expressed at about the same level? What are the most abundant two transcripts, and how much of the population to they account for? What are the least abundant two transcripts?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conclusion of Lestrade is a result of a poor implementation of EM. The 10 mRNA transcripts are definitely not expressed at about the same level \n",
    "\n",
    "Per our transcript abundances shown above, the two most abundand ones are H and G. The two least abundant ones are E and J."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
